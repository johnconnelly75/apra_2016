{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests # library for getting webpages (and a ton of other stuff, definitely something look into)\n",
    "from lxml import html # lxml is a XML parsing library, they have a submodule (look it up) called html that parses HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sidenote = \"\"\"\n",
    "HTML is a semi-structured data format. The data is hidden between tags and particularly useful is that the data \n",
    "we're grabbing exists in a HTML table so it's even more structured. But we must nevertheless extract it. Feel free to steal\n",
    "the below function for your own scraping. Or make it better. Or do whatever you want, I don't care.\n",
    "\n",
    "Scenario 1 (note the presence of tbody):\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td> item1 </td>\n",
    "            <td> item2 </td>\n",
    "            <td> item3 </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "xpath to get the table --> //table/tbody\n",
    "================================\n",
    "Scenario 2:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> item1 </td>\n",
    "        <td> item2 </td>\n",
    "        <td> item3 </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "xpath to get the table --> //table\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_table(htm_tbl):\n",
    "    rows = []\n",
    "    \n",
    "    # make sure the xpath object stored in the htm_tbl parameter and is passed into the function is at the table level\n",
    "    # if there is a tbody element then include that in the xpath so that you can just access the <tr> elements directly. \n",
    "    # depending on the author of the website, they might use the tbody or they might use just table and go directly into <tr>\n",
    "    \n",
    "    trs = htm_tbl.xpath('tr') # this is calling out all of the tr elements\n",
    "    for row in trs:\n",
    "        nR = [] # this is an empty list to be used below\n",
    "        tds = row.xpath('td')\n",
    "        for td in tds:\n",
    "            nR.append(td.text_content()) # this is doing two things! Can you guess what it is? (think order of operations even)\n",
    "            # it's taking the td variable, which is storing the td element, and is accessing the text attribute\n",
    "            # it is then appending that to the nR list we created\n",
    "        if len(nR) > 0:\n",
    "            rows.append(nR) # this is placing the nR list into the rows list making a list of lists!\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    # Here we're getting the data we need for below, but won't actually discuss web scraping until later...\n",
    "\n",
    "    url = r'https://en.wikipedia.org/wiki/List_of_members_of_the_Forbes_400'\n",
    "    r = requests.get(url) # this is using the get function in the requests library that gets the HTML of a website\n",
    "    print(r)\n",
    "    body = html.fromstring(r.content) # this is putting the content into a parsable format\n",
    "\n",
    "    xp = r'//*[@id=\"mw-content-text\"]/table[2]' # this is an xpath. Look it up.\n",
    "    tbl = body.xpath(xp)\n",
    "    \n",
    "    \n",
    "    data = extract_table(tbl[0])\n",
    "    \n",
    "    \n",
    "    # okay so it's not perfect, we still have to process it (the first element is wonky and the net worth isn't there)\n",
    "    # also you can handle the header in two ways you can tell python to grab it by accessing the <th> element or you can\n",
    "    # just hand write it like below:\n",
    "    header = ['rank', 'Name', 'Net worth (USD)', 'Sources of wealth']\n",
    "    # You'll notice the first element brought in a bit of noise, that's no fun\n",
    "    # what do you notice? Can you tell if there is a way to extract it?\n",
    "    # the easy thing to do since it's just a sequence is to replace it with the sequence and do away with that noise, \n",
    "    # but we'll entertain the idea that it's not that easy...\n",
    "\n",
    "    example = ['7000100000000000000♠1', 'Bill Gates', '$76\\xa0billion ', 'Microsoft, Cascade Investments LLC']\n",
    "    ['7000200000000000000♠2', 'Warren Buffett', '$62\\xa0billion ', 'Berkshire Hathaway']\n",
    "    ['7000300000000000000♠3', 'Larry Ellison', '$47.5\\xa0billion ', 'Oracle Corporation']\n",
    "\n",
    "    \n",
    "    rows = [] # this is an empty container to place the rows\n",
    "    \n",
    "    for row in data:\n",
    "        \n",
    "        if len(row) > 0:\n",
    "            row[0] = row[0].split('♠')[1] # this is cleaning the first element in the list\n",
    "\n",
    "            row[2] = row[2].replace('\\xa0billion ', '').replace('$', '')\n",
    "            row[2] = float(row[2]) * 1000000000    \n",
    "\n",
    "            rows.append(row)\n",
    "    return header, rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header, billionaires = getData()\n",
    "for billionaire in billionaires:\n",
    "    print(billionaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to explore all of these through a nifty searcher\n",
    "\n",
    "We'll be doing this through various functions (that we write) and manipulate the data. (And we're going to do it mostly live...)\n",
    "\n",
    "We'll be going on a journey from a single list of names with some attributes to a list of \n",
    "\n",
    "### First things first\n",
    "\n",
    "First, we have to go to Google and set up an API and what not and I'll show you how to do that. Keep in mind that this isn't necessarily free, but it's pretty cheap; allowing you to do 20+ searches per name! \n",
    "\n",
    "You can use this for farming which folks to focus on. Those with the most number of results on strict search parameters will be those you might want to focus on. \n",
    "\n",
    "### Down and dirty\n",
    "\n",
    "See below for a skeleton of functions...\n",
    "\n",
    "Now we're getting into programming! You previously learned just the syntax. The art of programming and the more difficult part is aligning your thinking appropriately. As researchers, we're pretty good at problem solving. We have to create something out of seemingly nothing. We take a breadcrumb and extrapolate the meaning from it with regard to a prospect. We are constantly moving from big picture to the granular and back. This is what programming is on a much more extreme scale. Programming is problem solving.\n",
    "\n",
    "In programming you have to deconstruct a problem into its smallest form, then solve that and then solve the next level and so on until the bigger question or problem is answered. \n",
    "\n",
    "Our problem, we have 200 names, and we want to search google for each of them, using all of the variations we can think of. \n",
    "\n",
    "What do we need to do first? What are we missing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_terms(row):\n",
    "    \n",
    "    id_ = row[0]\n",
    "    name = row[1]\n",
    "    name = '\"{}\"'.format(name) # this puts quotes around the search term\n",
    "    src = row[-1] # you can access the last element of a list by using -1\n",
    "    \n",
    "    search_terms = [\"{name} {src}\".format(name=name, src=src)]\n",
    "    \n",
    "    other_terms = [\n",
    "        'John Connelly', 'Rochester Institute of Technology', 'Art Institute of Chicago'\n",
    "        ] # this is a list for any other terms you'd like to add\n",
    "    \n",
    "    sites = [\n",
    "        'site:bloomberg.com', 'site:forbes.com'\n",
    "        ]\n",
    "    \n",
    "    for ot in other_terms:\n",
    "        item = '\"{}\"'.format(ot)\n",
    "        search_term = \"{name} {item}\".format(item=item, name=name)\n",
    "        search_terms.append(search_term)\n",
    "        \n",
    "    for s in sites:\n",
    "        item = '\"{}\"'.format(s)\n",
    "        search_term = \"{name} {item}\".format(item=item, name=name)\n",
    "        search_terms.append(search_term)\n",
    "    \n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_terms(billionaires[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_google_search(query, start=1):\n",
    "    api_key = '' # Get your own API key from google after enabling the custom serach API \n",
    "    cx = '001917876160815680181:4fyirrxtkq4'\n",
    "    u = r'https://www.googleapis.com/customsearch/v1'\n",
    "        \n",
    "    params = {\n",
    "            'cx': cx,\n",
    "            'q': query,\n",
    "            'key': api_key,\n",
    "            'start': start\n",
    "        }\n",
    "    r = requests.get(u, params=params)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def google_search(query, pages=5):\n",
    "    results = []\n",
    "    formatted_results = []\n",
    "    urls = []\n",
    "    cnt = 1\n",
    "    dbl = 0\n",
    "    for p in range(pages):\n",
    "\n",
    "        if p == 0:\n",
    "            p = 1\n",
    "        else:\n",
    "            p *= 10\n",
    "\n",
    "        google_results = basic_google_search(query, start=p)\n",
    "        items = google_results.get('items')\n",
    "        if items is not None:\n",
    "            for ix, res in enumerate(items):\n",
    "                res['query'] = query\n",
    "                results.append(res)\n",
    "                #print(res.keys())\n",
    "                link = res.get('link')\n",
    "                htmlFormattedUrl = res.get('htmlFormattedUrl')\n",
    "                htmlSnippet = res.get('htmlSnippet')\n",
    "                formattedUrl = res.get('formattedUrl')\n",
    "                htmlTitle = res.get('htmlTitle')\n",
    "                kind = res.get('kind')\n",
    "                displayLink = res.get('displayLink')\n",
    "                title = res.get('title')\n",
    "                cacheId = res.get('cacheId')\n",
    "                snippet = res.get('snippet')\n",
    "                pagemap = res.get('pagemap')\n",
    "\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "                    formatted_results.append([title, link, snippet, query])\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    dbl += 1\n",
    "                                    \n",
    "    return results, formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "header, billionaires = getData()\n",
    "\n",
    "search_terms = []\n",
    "for billionaire in billionaires:\n",
    "    print(billionaire[1])\n",
    "    terms = extract_terms(billionaire)\n",
    "    for term in terms:\n",
    "        print('\\t', term)\n",
    "    billionaire.append([]) # this places an empty list in the billionaire row to store our search terms\n",
    "    for row in terms:\n",
    "        billionaire[-1].append(row)\n",
    "    \n",
    "    search_terms.append(billionaire)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_results = []\n",
    "for entity in search_terms:\n",
    "    print(entity[:-1])\n",
    "    terms = entity[-1]\n",
    "    for term in terms:\n",
    "        print('\\t', term)\n",
    "        results, formatted_results = google_search(term, pages=2)\n",
    "        for res in formatted_results:\n",
    "            nR = entity[:-1] + res\n",
    "            google_results.append(nR)\n",
    "            print('\\t\\t', res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for gr in google_results:\n",
    "    print(gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = ['rank', 'name', 'net worth', 'source of wealth', 'search_title', 'search_url', 'search_blurb', 'search_term']\n",
    "df = pd.DataFrame(google_results, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('google_results_billionaires.xlsx')\n",
    "df.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
