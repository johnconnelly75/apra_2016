{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests # comes with Anaconda - great for interacting with websites and APIs\n",
    "from lxml import html # comes with Anaconda - great for parsing HTML\n",
    "import zipfile, io, os # standard lib\n",
    "import pandas as pd # awesome library | comes with Anaconda\n",
    "import re # part of standard lib\n",
    "import nameparser # you'll have to pip install nameparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 -- Analyzing wealthy families\n",
    "\n",
    "{GO OVER WEB SCRAPING FIRST}\n",
    "\n",
    "You found a great article online and they were nice enough to put it in a table on their website. How do you easily get the data?\n",
    "\n",
    "For something like this, you could easily just copy and paste into Excel, right? Well kind of. For simple tables and for easy things, sure that makes the most sense. No need to complicate gathering the data. You want to make that as efficient as possible, so do that. But let's say for this, that there are 100 pages of these names (I wish!). Or that you want to revisit them on a quarterly basis. Then doing the same thing and cleaning the same data in the same way each time doesn't make all that much sense when a couple of functions in Python can do. \n",
    "\n",
    "Or imagine you want to get the names, do whatever processing you want, add further context, compare against your database, and then store the data. Just copy and pasting into Excel is just one step, what if you didn't have to do that step? \n",
    "\n",
    "So, how should we think about this problem? What should our first step be? I know many of you, if not all of you, aren't familiar with Python, that's why you're in this session, but use what you do know to think about this. \n",
    "\n",
    "https://i.imgur.com/WRuJV6r.png\n",
    "\n",
    "### Thought Process (very granular)\n",
    "1. Do we have permission via the TOS (or rather are there any provisions forbidding access?) or the robots.txt?\n",
    "2. For wikipedia we do, as long as we're kind and don't hit their servers with too many requests too fast (this is common, basically don't be a jerk and respect their server)\n",
    "3. We're building a scraper, so we need something to scrape. We have to get the data somehow. We do this with the requests library and specifically we have to use the .get method. \n",
    "4. Once we have the web data, we have to parse it using the lxml.html library. (There are a couple of others namely, beautifulsoup). See below for how that works and read up about how to parse HTML with python. \n",
    "\n",
    "  * This is done by using selectors to extract bits of information.\n",
    "    * Think of it as traversing a tree (because you are) so parent --> child --> grandchild\n",
    "    * In trees, there are nodes, those nodes can have parents, siblings, children, grandchildren, and so on.\n",
    "    * Xpath (//div extracts all div elements)\n",
    "    * css selectors (div)\n",
    "5. We have to analyze the HTML for how to best write the selectors. Try to find common attributes in the HTML to extract the elements you want. That could be in class|id|attr name. If not, you'll have to find the lowest order parent that still leads to all of the data points you want to extract and start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getData():\n",
    "    url = r'https://en.wikipedia.org/wiki/List_of_wealthiest_families'\n",
    "    r = requests.get(url)\n",
    "    print(r)\n",
    "    body = html.fromstring(r.text)\n",
    "    tbl = body.xpath(r'//*[@id=\"mw-content-text\"]/table[3]')\n",
    "    trs = tbl[0].xpath('tr') # this creates a list of elements\n",
    "    rows = [] # this is an empty list to store all of the rows\n",
    "    for tr in trs:\n",
    "        # i use print statements as scaffolding to make sure i'm building in the right direction. \n",
    "        row = [] # this is an empty list to store our parsed row\n",
    "        urls = [] # this is an empty list for all of urls we'll be grabbing\n",
    "        tds = tr.xpath('td')\n",
    "        # below is a nested for loop\n",
    "        for td in tds:\n",
    "            spans = td.xpath('span')\n",
    "            if len(spans) > 0:\n",
    "                row.append(spans[1].text)\n",
    "            else:\n",
    "                row.append(td.text_content())\n",
    "                href = td.xpath('a')\n",
    "                if len(href) > 0:\n",
    "                    for h in href:\n",
    "                        url = \"https://en.wikipedia.org\" + h.get('href')\n",
    "                        urls.append(h.text)\n",
    "                        urls.append(url)\n",
    "\n",
    "        if len(row) > 0: # so if it's not empty\n",
    "            #print(row)\n",
    "            row += urls\n",
    "            rows.append(row)        \n",
    "            \n",
    "    # another direction to take this is to parse out the list of family members to do more research on... like maybe\n",
    "    # check if they're in your database...\n",
    "    \n",
    "    return rows\n",
    "rows = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_NU(url):\n",
    "    r = requests.get(url)\n",
    "    hits = re.findall('Northwestern University|Kellogg School of Management', r.text, re.I)\n",
    "    return len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    print(row[0])\n",
    "    for element in row:\n",
    "        if element.startswith('htt'):\n",
    "            hits = check_NU(element)\n",
    "            if hits > 0:\n",
    "                print('\\t[{}]'.format(hits), element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
